---
title: "Replicability Lab 2"
author: "Data Science Team"
output: 
     html_document:
         keep_md: true
---

```{r, echo=FALSE}
BH<-function(p,alpha)
  {
    pw<-na.omit(p)
    n<-length(pw)
    pw<-sort(pw)
    comp<-(pw<(1:n)*alpha/n)
    outcome<-sum(comp==TRUE)    
    if(outcome>0){
      last<-max((1:n)[comp==TRUE])
      pcut<-pw[last]
      shr<-p*0
      shr[p<=pcut]<-1
      out<-list(shr,sum(shr>0,na.rm=T),pcut)
      names(out)<-c("Reject","Total.Rej","Pcut")
    }
    else
      {
        shr<-p*0
        out<-list(shr,outcome,0)
        names(out)<-c("Reject","Total.Rej","Pcut")
      }
    return(out)
    
  }
```

## Today's Activity

Today's activity will investigate the winner's curse: that discoveries (even real ones!) sometimes lose their effect sizes as we replicate the experiments.  

### The winner's curse: regression towards the mean/shrinkage

First, we will generate data in two phases: first, we generate a "true" signal vector $v^{\rm true}$ of dimension $M = 1000$ where the entries of $v^{\rm true}$ are all normally distributed, that is, $v_i^{\rm true} \sim \mathcal{N}(0, 1)$. Then
we generate an observation $y$ with $y = v^{\rm true} + W$ where $W$ is a noise vector, each entry of $W$ also having a normal distribution $\mathcal{N}(0,1)$.

```{r, echo=TRUE}
M<-1000
v.true <- rnorm(M)
y <- rnorm(M, v.true, 1)
```

Now, based on these data, let us draw four plots. First, you should generate 2 plots, where the first is a plot of the true mean $v^{\rm true}$ against the observed value $y$. Your first task is to complete the code below:

```{r, echo=TRUE}
par(mfcol=c(1,2))  # Generate a new panel of plots
plot(v.true, y,
     xlab="True Means", ylab="Observations", main="Scatterplot")
points(v.true, v.true, col="blue", pch=19)
abline(0, 1, col=2, lwd=2)  # Plot 45 degree line

plot(v.true, y-v.true,
     xlab="True Means", ylab="y - v", main="Sorting by true means")
abline(0,0,col="red",lwd=2)
```

Now, let us change around the plot order. In particular, you should plot the true means $v^{\rm true}$ on the vertical axis against the observed means $y$ on the horizontal axis.

```{r, echo=TRUE}
par(mfcol = c(1,2))
plot(y, v.true,
     ylab="True Means v", xlab="Observations y", main="Flipped plot and 45 degree line")
abline(0, 1, col=2,lwd=2) # Plot 45 degree line

plot(y, y-v.true,
     xlab="Observation y", ylab="y - v", main="Sorting by observations")
abline(0,0,col="red",lwd=2)  # Plot horizontal line
```

**Task**
After you have implemented the code above, explain what you are observing.

### The winner's curse revisited: the effect of selective reporting

Now that we have seen some results on regression toward the mean, let us consider the effect of selective reporting, that is, that we can only publish results with p-values $p < \alpha$, where $\alpha$ is our pre-chosen significance level (e.g., .01 or .00001 or .05).

To that end, we generate $M = 1000$ datapoints, each from a normal distribution with mean $\mu = .3$. Thus, for each hypothesis, the null $H_0 : \mu = 0$ is false. We will then report whether $p < .05$ for this null, plotting the results.
(We observe $y = \mu + \mathcal{N}(0, 1)$ for each of 1000 dimensions.)

```{r, echo=FALSE}
par(mfrow=c(1,3))
Tmean<-rep(0,1000) # the majority of means is equal to 0
N<-1000 # number of non zero means
mu<-0.3 # signal strength
Tmean[1:N]<-mu # first N means are equal to mu 
y<-rnorm(1000,Tmean,1)
pvalue<-2*pnorm(-abs(y))
discoveries<-pvalue<0.05
lower<-y-1.96
upper<-y+1.96

plot(y,pch=20,main="All variables",ylim=c(min(lower),max(upper)),xlab="")
for(i in 1:length(discoveries))
{
	{lines(c(i,i),c(lower[i],upper[i]),col=2)}
}

mtext(paste("true signal =",as.character(mu)),side=3,line=0)
points(1:1000,y,pch=20)
points(1:1000,Tmean,col="blue",pch=19)
mtext(paste("Average signal =",as.character(round(mean(y[1:N]),3))),side=1,line=2.5)
mtext(paste("Percent. of means covered =",as.character(round(sum((Tmean>=lower)*(Tmean<=upper))/1000,3))),side=1,line=4)

plot(y,pch=20,main="Report if p-value<0.05",ylim=c(min(lower),max(upper)),xlab="")
lower<-y-1.96
upper<-y+1.96
for(i in 1:length(discoveries))
{
	if(discoveries[i]==1)
	{lines(c(i,i),c(lower[i],upper[i]),col=2)}
}
points(1:1000,y,pch=20)
points(1:1000,Tmean,col="blue",pch=19)
mtext(paste("Percent. correct sign  =",as.character(round(mean(y[discoveries==1]>=0),3))),side=1,line=2.5)

mtext(paste("Percent. of means covered =",as.character(round(sum((Tmean[discoveries==1]>=lower[discoveries==1])*(Tmean[discoveries==1]<=upper[discoveries==1]))/sum(discoveries==1),3))),side=1,line=4)


dy<-y[discoveries==1]
dlower<-lower[discoveries==1]
dupper<-upper[discoveries==1]
plot(dy,pch=20,main="Zooming on reports",ylim=c(min(lower),max(upper)),xlab="")
for(i in 1:length(dy))
{
	
	lines(c(i,i),c(dlower[i],dupper[i]),col=2)
}
points(1:length(dy),dy,pch=20)
points(1:length(dy),Tmean[discoveries==1],col="blue",pch=19)
mtext(paste("Average positive reports=",as.character(round(mean(dy[dy>0]),3))),side=1,line=2.5)
mtext(paste("Average negative reports ",as.character(round(mean(dy[dy<0]),3))),side=1,line=4)
```

After this code, the vector `dy` stores the $y$ values for the "discoveries", that is, the rejected null hypotheses.

**Task**

Calculate the mean values of $y$ on the rejected nulls with positive and negative $y$ values, respectively. What are these values?

```{r, echo=FALSE}
mean(dy[dy > 0])
mean(dy[dy < 0])
```

How much larger are the positive $y$ "discoveries" than the true mean $\mu$?

### Winner's curse again: bias and confidence interval coverage

Now, we will use a clever procedure known as the Benjamini-Hochberg step-up procedure (we will discuss this in class on Friday) that provides some correction for the fact that we are doing many tests.
In this case, we generate $M = 1000$ experiments, the first $N = 100$ of which have mean $\mu = 3$ and the remainder of which are all true nulls, that is, $\mu = 0$.

```{r, echo=TRUE}
Tmean<-rep(0,1000) # the majority is equal to 0
N<-100 # number of non zero means
mu<-3 # signal strength
Tmean[1:N]<-mu
y<-rnorm(1000,Tmean,1)
pvalue<-2*pnorm(-abs(y))
# Now, use Benjamini-Hochberg to get discoveries
discoveries<-BH(pvalue,0.05)[[1]]
discoveries
lower<-y-1.96
upper<-y+1.96
par(mfrow=c(1,3))
plot(y,pch=20,main="All",ylim=c(min(lower),max(upper)),xlab="")
for(i in 1:length(discoveries))
{
	{lines(c(i,i),c(lower[i],upper[i]),col=2)}
}
points(1:1000,y,pch=20)
points(1:1000,Tmean,col="blue",pch=19)
mtext(paste("Percent. of means covered =",as.character(round(sum((Tmean>=lower)*(Tmean<=upper))/1000,3))),side=1,line=3)

plot(y,pch=20,main="Selected",ylim=c(min(lower),max(upper)),xlab="")
for(i in 1:length(discoveries))
{
	if(discoveries[i]==1)
	{lines(c(i,i),c(lower[i],upper[i]),col=2)}
}
points(1:1000,y,pch=20)
points(1:1000,Tmean,col="blue",pch=19)
mtext(paste("Percent. of means covered =",as.character(round(sum((Tmean[discoveries==1]>=lower[discoveries==1])*(Tmean[discoveries==1]<=upper[discoveries==1]))/sum(discoveries==1),3))),side=1,line=3)



dy<-y[discoveries==1]
dlower<-lower[discoveries==1]
dupper<-upper[discoveries==1]
plot(dy,pch=20,main="Zooming on selected",ylim=c(min(lower),max(upper)),xlab="")
for(i in 1:length(dy))
{
	
	lines(c(i,i),c(dlower[i],dupper[i]),col=2)
}
points(1:length(dy),dy,pch=20)
points(1:length(dy),Tmean[discoveries==1],col="blue",pch=19)
mtext(paste("Percent. of means covered =",as.character(round(sum((Tmean[discoveries==1]>=lower[discoveries==1])*(Tmean[discoveries==1]<=upper[discoveries==1]))/sum(discoveries==1),3))),side=1,line=3)
```

**Task**

Explain the plots just displayed. What is good about the discoveries you make? What is bad?
